

## DyCodeEval

DyCodeEval is a novel dynamic benchmarking suite that employs multiple agents to generate a set of diverse semantically equivalent problems in order to provide transparent contamination-free evaluation on code LLMs. Below is an example of a problem that was generated by DyCodeEval from the seed programming problem.


The following Figure shows an example of applying DyCodeEval to generate a new programming problem description from HumanEval.
<div  align="center">    
 <img src="https://github.com/anonymousGithub2022/DyCodeEval/blob/main/resource/example%20(1)-page-001.jpg" width="1200" height="320" alt="An Example from DyCodeEval"/><br/>
</div>   


### Benchmarking on-the-wild Model
<div  align="center">    
 <img src="https://github.com/anonymousGithub2022/DyCodeEval/blob/main/resource/wild.jpg" width="1200" height="320" alt="An Example from DyCodeEval"/><br/>
</div>   


### Stability of DyCodeEval

<div  align="center">    
 <img src="https://github.com/anonymousGithub2022/DyCodeEval/blob/main/resource/stability.jpg" width="1200" height="320" alt="Benchmarking on-the-wild model"/><br/>
</div>   
